# -*- coding: utf-8 -*-
"""model_training.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FyRwWy_zxsjlq9QhpjXOaOKngfKCnSFc
"""

import tensorflow as tf
from tensorflow.keras.layers import (
    Input, Conv2D, BatchNormalization, Activation, MaxPooling2D,
    GlobalAveragePooling2D, Dense, Dropout, LSTM, Bidirectional,
    Reshape, Attention, GRU, Flatten
)
from tensorflow.keras.callbacks import (
    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard
)
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import numpy as np
import pandas as pd

def create_cnn_model(input_layer):
    """Create a CNN model architecture."""
    conv1 = Conv2D(512, kernel_size=(5, 5), strides=(1, 1), padding='same')(input_layer)
    bn1 = BatchNormalization()(conv1)
    relu1 = Activation('relu')(bn1)
    pool1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(relu1)

    conv2 = Conv2D(256, kernel_size=(5, 5), dilation_rate=(2, 2), padding='same')(pool1)
    bn2 = BatchNormalization()(conv2)
    relu2 = Activation('relu')(bn2)
    pool2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(relu2)

    conv3 = Conv2D(128, kernel_size=(3, 3), dilation_rate=(2, 2), padding='same')(pool2)
    bn3 = BatchNormalization()(conv3)
    relu3 = Activation('relu')(bn3)
    pool3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(relu3)

    conv4 = Conv2D(64, kernel_size=(3, 3), padding='same')(pool3)
    bn4 = BatchNormalization()(conv4)
    relu4 = Activation('relu')(bn4)
    pool4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(relu4)

    global_avg_pooling = GlobalAveragePooling2D()(pool4)
    return global_avg_pooling

def create_lstm_model(input_layer):
    """Create an LSTM model architecture."""
    lstm1 = LSTM(256, return_sequences=True)(input_layer)
    lstm2 = LSTM(128, return_sequences=True)(lstm1)
    lstm3 = LSTM(64)(lstm2)

    dropout = Dropout(0.3)(lstm3)
    return dropout

def add_attention_layer(layer):
    """Add attention layer to the model."""
    attention = Attention()([layer, layer])
    return attention

def create_and_compile_model(input_layer, output_layer, optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):
    """Create and compile a model."""
    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
    return model

def create_multi_head_attention(layer, heads=8):
    """Create multi-head attention layer."""
    attention_heads = []
    for _ in range(heads):
        attention_head = Attention()([layer, layer])
        attention_heads.append(attention_head)
    merged_attention = tf.keras.layers.Concatenate(axis=-1)(attention_heads)
    return merged_attention

def train_models(X_train, y_train, X_test, y_test, drive_path='/content/drive/My Drive/ensemble_full_prod/ensemble_full/full_prod_ser_'):
    """Train and evaluate multiple models."""
    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2], 1))

    # Create Model 1: CNN, LSTM, GRU with Attention
    cnn_output = create_cnn_model(input_layer)
    reshape1 = Reshape((1, 64))(cnn_output)  # Adjust the shape
    bidirectional_lstm = Bidirectional(LSTM(128, return_sequences=True))(reshape1)
    attention_lstm = add_attention_layer(bidirectional_lstm)
    bidirectional_gru = Bidirectional(GRU(64, return_sequences=True))(attention_lstm)
    attention_gru = add_attention_layer(bidirectional_gru)
    attention_flatten = Flatten()(attention_gru)
    output_layer_1 = Dense(units=8, activation='softmax')(attention_flatten)
    model1 = create_and_compile_model(input_layer, output_layer_1)

    # Create Model 2: Only CNN
    cnn_output_2 = create_cnn_model(input_layer)
    output_layer_2 = Dense(units=8, activation='softmax')(cnn_output_2)
    model2 = create_and_compile_model(input_layer, output_layer_2)

    # Create Model 3: CNN and LSTM
    cnn_output_3 = create_cnn_model(input_layer)
    reshape_3 = Reshape((4, 16))(cnn_output_3)
    bidirectional_lstm_3 = Bidirectional(LSTM(128, return_sequences=True))(reshape_3)
    attention_lstm_3 = add_attention_layer(bidirectional_lstm_3)
    flatten_3 = Flatten()(attention_lstm_3)
    output_layer_3 = Dense(units=8, activation='softmax')(flatten_3)
    model3 = create_and_compile_model(input_layer, output_layer_3)

    # Create Model 4: CNN and GRU
    cnn_output_4 = create_cnn_model(input_layer)
    output_layer_4 = Dense(units=8, activation='softmax')(cnn_output_4)
    model4 = create_and_compile_model(input_layer, output_layer_4)

    # Create Model 5 with Multi-Head Attention: CNN, Bidirectional LSTM, Bidirectional GRU
    cnn_output_5 = create_cnn_model(input_layer)
    reshape_5 = Reshape((1, 64))(cnn_output_5)
    bidirectional_lstm_5 = Bidirectional(LSTM(128, return_sequences=True))(reshape_5)
    bidirectional_gru_5 = Bidirectional(GRU(64, return_sequences=True))(bidirectional_lstm_5)
    multi_head_attention_5 = create_multi_head_attention(bidirectional_gru_5, heads=4)
    attention_flatten_5 = Flatten()(multi_head_attention_5)
    output_layer_5 = Dense(units=8, activation='softmax')(attention_flatten_5)
    model5 = create_and_compile_model(input_layer, output_layer_5)

    models = [model1, model2, model3, model4, model5]
    model_names = ['model1', 'model2', 'model3', 'model4', 'model5']
    histories = []  # List to store training histories

    for i, model in enumerate(models):
        full_model_path = f"{drive_path}{model_names[i]}.keras"
        print("Full Model Path for", model_names[i], ":", full_model_path)
        print("Model Summary for", model_names[i], ":")
        model.summary()

        # Callbacks
        early_stopping = EarlyStopping(
            monitor='val_loss',
            min_delta=0.001,
            patience=20,
            verbose=1,
            mode='min',
            restore_best_weights=True
        )

        rlrp = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.4,
            patience=20,
            verbose=1,
            min_lr=0.0000001
        )

        model_checkpoint = ModelCheckpoint(
            full_model_path,
            monitor='val_loss',
            verbose=1,
            save_best_only=True,
            mode='min'
        )

        tensorboard_callback = TensorBoard(log_dir='logs_'+model_names[i], histogram_freq=1)

        # Train the model
        history = model.fit(
            X_train, y_train,
            batch_size=32,
            epochs=50,
            validation_data=(X_test, y_test),
            callbacks=[early_stopping, rlrp, model_checkpoint, tensorboard_callback]
        )

        # Append the history to the list
        histories.append(history)

    return models, histories

# usage:
# models, histories = train_models(X_train, y_train, X_test, y_test)