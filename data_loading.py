# -*- coding: utf-8 -*-
"""data_loading.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JmaL30auUsqzPzH9pA-CE66n8zFPDor8
"""

import numpy as np
import pandas as pd
import librosa
import joblib
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
import concurrent.futures
from tqdm import tqdm
from feature_extraction import extract_features  # Import the existing extract_features function
from audio_augmentation import add_background_noise, stretch, shift, add_echo, add_reverb  # Import augmentation functions

# Function to normalize audio
def normalize_audio(audio):
    original_max = np.max(np.abs(audio))
    return audio / original_max

# Function to trim silence from audio
def trim_silences(data):
    return librosa.effects.trim(data)[0]

# Function to ensure audio is exactly 3 seconds long
def pad_or_truncate(audio, target_duration=3.0, sample_rate=16000):
    target_length = int(target_duration * sample_rate)
    if len(audio) > target_length:
        return audio[:target_length]  # Truncate
    else:
        return np.pad(audio, (0, target_length - len(audio)), 'constant')  # Pad

# Function to process and extract features from audio files
def get_features(path, background_noise_path='path/to/background_noise'):
    try:
        data, sample_rate = librosa.load(path, sr=16000)
        trimmed_data = trim_silences(data)
        normalized_data = normalize_audio(trimmed_data)
        padded_data = pad_or_truncate(normalized_data)

        features_list = []

        # Extract features from normalized audio
        features_list.append(extract_features(padded_data, sample_rate))

        # Augmentations
        noise_data = add_background_noise(padded_data, background_noise_path)
        features_list.append(extract_features(noise_data, sample_rate))

        stretched_data = stretch(padded_data)
        features_list.append(extract_features(stretched_data, sample_rate))

        shifted_data = shift(padded_data)
        features_list.append(extract_features(shifted_data, sample_rate))

        echo_data = add_echo(padded_data, sample_rate)
        features_list.append(extract_features(echo_data, sample_rate))

        reverb_data = add_reverb(echo_data, sample_rate)
        features_list.append(extract_features(reverb_data, sample_rate))

        # Stack all features into a single array
        return np.vstack(features_list)

    except Exception as e:
        print(f"Error processing file: {path}")
        print(f"Error details: {str(e)}")
        return None

# Function to process files in parallel
def process_file(file_info):
    path, emotion = file_info
    features = get_features(path)
    return features, emotion if features is not None else None

# Function to load datasets and extract features
def load_datasets(dataframes):
    X, Y = [], []
    for df in dataframes:
        total_files = len(df.Path)
        file_info_list = list(zip(df.Path, df.Emotions))

        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = list(tqdm(executor.map(process_file, file_info_list), total=total_files))

        for result in results:
            if result is not None:
                features, emotion = result
                X.append(features)
                Y.append(emotion)

    return np.array(X), np.array(Y)

# Function to prepare data for training and testing
def prepare_data(df):
    X, y = load_datasets([df])

    # Encode labels
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=0, stratify=y_encoded)

    # Standardize the data
    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Save the scaler and label encoder
    joblib.dump(scaler, 'scaler.joblib')
    joblib.dump(label_encoder, 'label_encoder.joblib')

    return X_train, X_test, y_train, y_test